#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Inferenza Mask R-CNN su tutte le immagini in 'unlabeled img/' con maschere
ben visibili: fill opaco, contorno doppio (halo nero + bordo colorato) e
stampa area per ogni istanza. Una sola finestra per immagine.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import torch
from PIL import Image
from PIL.ExifTags import TAGS
from torchvision.transforms import functional as F
from torchvision.models.detection import maskrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor


# =========================
# ========= Config ========
# =========================

IMAGE_DIR = Path("test img")
MODEL_PATH = Path("modelli") / "maskrcnn_organoid.pth"
COCO_PATH = Path("augmented") / "augmented_coco.json"  # opzionale per nomi classe

NUM_CLASSES = 2          # background + 1 classe (aggiorna se necessario)
SCORE_THRESHOLD = 0.5    # confidenza minima per mostrare/contare

# Aspetto overlay
FILL_ALPHA = 0.65                # opacit√† del riempimento maschera
OUTLINE_WIDTH = 3.0              # spessore bordo colorato
OUTLINE_HALO_WIDTH = 6.0         # spessore halo nero sotto al bordo
OUTLINE_HALO_COLOR = "black"     # colore halo per contrasto





# =========================
# ===== Retrieve Metadata ======
# =========================

def get_pixel_info(image_file_path):
    """
    Estrae la dimensione lineare e l'area di un singolo pixel
    dai metadati EXIF dell'immagine.

    Parametri:
        image_file_path (str): Percorso al file immagine.

    Ritorna:
        dict: {
            "pixel_size_um": float,
            "pixel_area_um2": float
        } oppure None se i dati non sono disponibili.
    """
    try:
        image = Image.open(image_file_path)
        exif_data = image._getexif()
        if not exif_data:
            print("Nessun dato EXIF trovato.")
            return None

        readable_exif = {TAGS.get(tag, tag): value for tag, value in exif_data.items()}
        x_res = readable_exif.get("FocalPlaneXResolution")
        res_unit = readable_exif.get("FocalPlaneResolutionUnit")

        if not x_res or not res_unit:
            print("Dati di risoluzione del piano focale mancanti.")
            return None

        # Calcolo dimensione pixel (lato) in micrometri
        if res_unit == 2:  # Inch
            pixel_size_um = (1 / x_res) * 25.4 * 1000
        elif res_unit == 3:  # Centimetri
            pixel_size_um = (1 / x_res) * 10 * 1000
        else:
            print(f"Unit√† di misura non supportata (code: {res_unit})")
            return None

        # Calcolo area pixel in micrometri quadrati
        pixel_area_um2 = pixel_size_um ** 2

        return {
            "pixel_size_um": pixel_size_um,
            "pixel_area_um2": pixel_area_um2
        }

    except Exception as e:
        print(f"Errore durante la lettura dell'immagine: {e}")
        return None


# =========================
# ===== Utilit√† COCO ======
# =========================

def load_contiguous_class_names(coco_path: Path) -> List[str]:
    """Restituisce i nomi classe indicizzati come il modello (0=background)."""
    if not coco_path.exists():
        return ["background"] + [f"class_{i}" for i in range(1, NUM_CLASSES)]

    with coco_path.open("r", encoding="utf-8") as f:
        coco = json.load(f)

    cats = coco.get("categories", [])
    cats_sorted = sorted(cats, key=lambda c: c["id"])
    names = ["background"] + [c.get("name", f"class_{i+1}") for i, c in enumerate(cats_sorted)]

    if len(names) != NUM_CLASSES:  # adatta con prudenza
        names = names[:NUM_CLASSES] if len(names) >= NUM_CLASSES else names + [
            f"class_{i}" for i in range(len(names), NUM_CLASSES)
        ]
    return names


# =========================
# ===== Modello / Vis =====
# =========================

def build_model(num_classes: int) -> torch.nn.Module:
    """Crea Mask R-CNN con classificatori sostituiti per num_classes."""
    model = maskrcnn_resnet50_fpn(weights="DEFAULT")
    in_feat = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_feat, num_classes)
    in_feat_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_feat_mask, 256, num_classes)
    return model


def overlay_fill_only(
    image_np: np.ndarray,
    masks: np.ndarray,
    scores: np.ndarray,
    labels: np.ndarray,
    class_names: List[str],
    score_thr: float,
    area_info: dict,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Applica SOLO il riempimento delle maschere (blend) sull'immagine e
    stampa area (px e %) per ogni istanza mantenuta.
    NON crea figure. Ritorna:
      - img_out (con fill)
      - masks_kept (ordinati per score)
      - labels_kept
      - scores_kept
    """
    h, w = image_np.shape[:2]
    total_px = float(h * w)

    kept = scores >= score_thr
    masks = masks[kept]
    scores = scores[kept]
    labels = labels[kept]

    if len(masks) == 0:
        print("‚Äî Nessuna maschera sopra soglia ‚Äî")
        return image_np, masks, labels, scores

    # Ordina per score decrescente (solo per log)
    order = np.argsort(scores)[::-1]
    masks = masks[order]
    scores = scores[order]
    labels = labels[order]

    img_out = image_np.copy()
    cmap = plt.get_cmap("tab20")

    for i in range(len(masks)):
        color_rgb = np.array(cmap(i % cmap.N))[:3]  # [0..1, 0..1, 0..1]
        mask_bin = masks[i] > 0.5

        # Area
        area_px = int(mask_bin.sum())
        area_pct = (area_px / total_px) * 100.0

        cls_idx = int(labels[i])
        cls_name = class_names[cls_idx] if 0 <= cls_idx < len(class_names) else f"id_{cls_idx}"
        print(f"   ‚Ä¢ {cls_name}: score={scores[i]:.2f}  area={area_px*area_info['pixel_area_um2']} um2  ({area_pct:.2f}%)")

        # Riempimento (blend)
        img_out[mask_bin] = (
            img_out[mask_bin] * (1.0 - FILL_ALPHA) + color_rgb * 255.0 * FILL_ALPHA
        ).astype(np.uint8)

    return img_out, masks, labels, scores


def show_and_wait(
    img_np: np.ndarray,
    window_title: str,
    masks: np.ndarray | None = None,
    labels: np.ndarray | None = None,
    scores: np.ndarray | None = None,
    class_names: List[str] | None = None,
) -> bool:
    """
    Mostra una sola finestra:
      - Visualizza img_np (gi√† con fill)
      - Se masks/labels/scores sono forniti: disegna contorni (halo + bordo) e label
      - Attende:
          qualsiasi tasto -> continua (return False)
          ESC o 'q'       -> esce (return True)
    """
    want_exit = {"flag": False}

    fig, ax = plt.subplots(figsize=(10, 10))
    ax.imshow(img_np)
    ax.axis("off")
    fig.suptitle(window_title, fontsize=12)
    fig.tight_layout()

    if masks is not None and len(masks) > 0:
        cmap = plt.get_cmap("tab20")
        for i in range(len(masks)):
            z = (masks[i] > 0.5).astype(float)
            color_rgb = np.array(cmap(i % cmap.N))[:3]
            # Halo + bordo
            ax.contour(z, levels=[0.5], colors=[OUTLINE_HALO_COLOR],
                       linewidths=OUTLINE_HALO_WIDTH, alpha=0.95)
            ax.contour(z, levels=[0.5], colors=[color_rgb],
                       linewidths=OUTLINE_WIDTH, alpha=1.0)
            # Etichetta al centroide
            ys, xs = np.nonzero(z > 0.5)
            if len(xs) > 0 and class_names is not None and labels is not None and scores is not None:
                cx, cy = int(xs.mean()), int(ys.mean())
                cls_idx = int(labels[i])
                cls_name = class_names[cls_idx] if 0 <= cls_idx < len(class_names) else f"id_{cls_idx}"
                ax.text(
                    cx, cy,
                    f"{cls_name} {float(scores[i]):.2f}",
                    ha="center", va="center",
                    fontsize=10, color="white",
                    bbox=dict(facecolor="black", alpha=0.6, pad=2),
                )

    def on_key(event):
        key = (event.key or "").lower()
        if key in ("escape", "esc", "q"):
            want_exit["flag"] = True
        plt.close(fig)

    fig.canvas.mpl_connect("key_press_event", on_key)
    plt.show()  # blocca finch√© chiudi con un tasto
    return want_exit["flag"]


# =========================
# ========== Main =========
# =========================

def main() -> None:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üñ•Ô∏è Device: {device}")

    if not MODEL_PATH.exists():
        raise FileNotFoundError(f"Modello non trovato: {MODEL_PATH}")

    image_paths = sorted(
        [*IMAGE_DIR.glob("*.jpg"), *IMAGE_DIR.glob("*.jpeg"), *IMAGE_DIR.glob("*.png")]
    )
    if not image_paths:
        raise FileNotFoundError(f"Nessuna immagine trovata in: {IMAGE_DIR.resolve()}")

    # Modello
    model = build_model(NUM_CLASSES)
    state = torch.load(MODEL_PATH, map_location=device)
    model.load_state_dict(state)
    model.to(device).eval()

    # Nomi classe
    class_names = load_contiguous_class_names(COCO_PATH)

    print(f"‚û°Ô∏è  Trovate {len(image_paths)} immagini. Premi un tasto per avanzare; ESC/q per uscire.\n")

    for img_path in image_paths:
        print(f"\nüì∑ {img_path.name}")
        info = get_pixel_info(img_path)
        image_pil = Image.open(img_path).convert("RGB")
        image_tensor = F.to_tensor(image_pil).unsqueeze(0).to(device)

        with torch.no_grad():
            output = model(image_tensor)[0]

        masks = output["masks"].cpu().numpy()[:, 0, ...]   # [N, H, W]
        labels = output["labels"].cpu().numpy()
        scores = output["scores"].cpu().numpy()

        # Applica SOLO il fill e prepara dati per disegnare i contorni in un'unica finestra
        img_np = np.array(image_pil)
        img_filled, kept_masks, kept_labels, kept_scores = overlay_fill_only(
            image_np=img_np,
            masks=masks,
            scores=scores,
            labels=labels,
            class_names=class_names,
            score_thr=SCORE_THRESHOLD,
            area_info=info
        )

        # Mostra immagine finale (una sola volta) e attende input
        title = f"{img_path.name} ‚Äî premi un tasto per continuare, ESC/q per uscire"
        if show_and_wait(
            img_np=img_filled,
            window_title=title,
            masks=kept_masks,
            labels=kept_labels,
            scores=kept_scores,
            class_names=class_names,
        ):
            print("üëã Uscita richiesta dall'utente.")
            break


if __name__ == "__main__":
    main()
